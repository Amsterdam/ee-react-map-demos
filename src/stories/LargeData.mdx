import { Meta } from "@storybook/blocks";

<Meta title="Handling large volumes of data" />

# Handling large volumes of data

Maps commonly run into performance issues when handling with large volumes of data. Traditionally this is because with map libraries and JavaScript, each marker requires a corresponding DOM element.

This is less so an issue when using WebGL maps, as they make use of the HTML canvas element. However, dropping large volumes of data on the frontend should always be avoided as it can lead to varying issues of performance and with some (particularly legacy) devices buggy applications.

## Clustering

Clustering is the most common and often simplest solution for large datasets.

- [supercluster](https://github.com/mapbox/supercluster)
- [Leaflet.markercluster](https://github.com/Leaflet/Leaflet.markercluster)

## A full end-to-end solution

The large data problem is primarily a backend problem that cannot be solved solely on the frontend side; therefore, we need to restrict and limit geo-data sent to the frontend. This requires validating the frontendâ€™s map state and defining rules for each dataset, for example:

- Dataset has a high number of records (for example, 1 million+ records)
- Dataset has high volume data in a small locality (data is specific to a buurt, stadsdeel, wijk etc.)

Therefore, data should only be displayed when the zoom level is high, otherwise:

1. The user will be overloaded with data.
2. There is too much data to ever present efficiently.
3. Rendering too many map points can lead to undesireable performance.

Combining and utilizing common parameters from the frontend helps minimize the amount of data for a specific area. We can utilize features such as zoom levels and the map's current bounding box to avoid loading unnecessary out-of-view data or too much data.